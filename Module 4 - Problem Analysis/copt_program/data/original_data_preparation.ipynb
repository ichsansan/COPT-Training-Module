{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-24 10:16:51.282023: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-24 10:16:51.282052: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "if \"..\" not in sys.path:\n",
    "    sys.path.append(\"..\")\n",
    "if \"../contracts\" not in sys.path:\n",
    "    sys.path.append(\"../contracts\")\n",
    "\n",
    "from file_manager_helper import load_dataset_tag_contract, save_pandas_dataset\n",
    "from reducers.dataset_contract_reducers import convert_dataset, modelling_filter\n",
    "import matplotlib.pyplot as plt\n",
    "import config\n",
    "\n",
    "DATE_FORMAT: str = '%Y-%m-%d %H:%M:%S'\n",
    "OUTPUT_DIR: str = \"prepared_datasets\"\n",
    "\n",
    "def save_test_system_dataset(\n",
    "    dataset: pd.DataFrame,\n",
    "    dataset_tag: str,\n",
    "    output_dataset_name: str):\n",
    "\n",
    "    # Save data for testing the system\n",
    "    real_time_sample = dataset.tail(20000)\n",
    "    real_time_sample.index.rename(\"timestamp\", inplace=True)\n",
    "    real_time_sample.to_csv(os.path.join(OUTPUT_DIR, dataset_tag, f\"BAT_realtime_dataset_{output_dataset_name}.csv\"))\n",
    "\n",
    "def convert_and_save_dataset(\n",
    "    dataset: pd.DataFrame,\n",
    "    dataset_tag: str,\n",
    "    output_dataset_name: str,\n",
    "    output_dataset_name_suffix: str,\n",
    "    model_converters: Optional[Dict[str, Any]] = None,\n",
    "    model_converters_association_tags: Optional[Dict[str, Any]] = None,\n",
    "    act_on_na: bool = True):\n",
    "\n",
    "    # Loading the dataset contract\n",
    "    contract = load_dataset_tag_contract(\n",
    "        contract_alias=dataset_tag,\n",
    "         directory=os.path.join(\"..\", config.ZIP_DATASET_CONTRACTS_DIRECTORY, dataset_tag))\n",
    "\n",
    "    # Convert the dataset\n",
    "    converted_dataset, dataset = convert_dataset(\n",
    "        contract=contract,\n",
    "        dataset=dataset,\n",
    "        model_converters=model_converters,\n",
    "        model_converters_association_tags=model_converters_association_tags,\n",
    "        act_on_na=act_on_na)\n",
    "    \n",
    "    # Filter based on conditions of boiler being active\n",
    "    #converted_dataset = modelling_filter(\n",
    "    #                        contract=contract,\n",
    "    #                        dataset=converted_dataset)\n",
    "\n",
    "    # Plotting the efficiency\n",
    "    if \"Efficiency\" in set(converted_dataset.columns):\n",
    "        efficiency_values = converted_dataset[\"Efficiency\"].values\n",
    "\n",
    "        plt.plot(range(len(efficiency_values)), efficiency_values)\n",
    "        plt.show()\n",
    "        \n",
    "    converted_dataset.index.names=[\"date_rec\"]\n",
    "    # Prepare the converted dataset for saving\n",
    "    dataset = pd.melt(converted_dataset.reset_index(), id_vars='date_rec', value_vars=converted_dataset.columns, var_name='address_no', value_name='value')\n",
    "    #dataset.rename(columns={'index': 'date_rec'}, inplace=True)\n",
    "    # Make sure that the date is in the correct format by converting it.\n",
    "    dataset['date_rec'] = dataset['date_rec'].dt.strftime(DATE_FORMAT)\n",
    "    # Remove empty values from the dataset\n",
    "    dataset['value'].replace('', np.nan, inplace=True)\n",
    "    dataset.dropna(subset=['value'], inplace=True)\n",
    "    \n",
    "    \n",
    "    # Save data for training the models\n",
    "    save_pandas_dataset(dataset=dataset, dataset_tag=output_dataset_name, dataset_suffix=output_dataset_name_suffix, directory=OUTPUT_DIR)\n",
    "    \n",
    "def concatenate_daily(dir_path: str):\n",
    "    dataset_to_concat = []\n",
    "    \n",
    "    for dir in os.listdir(dir_path):\n",
    "        daily_df = pd.read_csv(os.path.join(dir_path,dir))\n",
    "        # Appending dataframes to concatenate\n",
    "        dataset_to_concat.append(daily_df)\n",
    "    total_df = pd.concat(dataset_to_concat)\n",
    "    return total_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCT 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('original_format/efficiency_modelling/PCT1/pct_1_efficiency_model.pickle', 'rb') as handle:\n",
    "#    pct_1_efficiency_calculator = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_1 = pd.read_csv(\"provided_data/datasets/PCT1.csv\", parse_dates= [\"timestamp\"])\n",
    "dataset_1.rename(columns={\"timestamp\":\"date_rec\"}, inplace=True)\n",
    "\n",
    "sensor_df = dataset_1.set_index(\"date_rec\")\n",
    "# Drop NAs only if for a row there are only NANs\n",
    "sensor_df = sensor_df.dropna(how=\"all\")\n",
    "sensor_df = sensor_df[~sensor_df.index.duplicated(keep='first')]\n",
    "\n",
    "sensor_df.to_csv(\"PCT1_for_sanity_checking_1.csv\")\n",
    "\n",
    "save_test_system_dataset(\n",
    "    dataset=sensor_df,\n",
    "    dataset_tag=config.PCT1_DATA_TAG,\n",
    "    output_dataset_name=\"pct1\")\n",
    "\n",
    "convert_and_save_dataset(\n",
    "    dataset=sensor_df,\n",
    "    dataset_tag=config.PCT1_DATA_TAG,\n",
    "    output_dataset_name=config.PCT1_DATA_TAG,\n",
    "    output_dataset_name_suffix=\"1\",\n",
    "    act_on_na=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1 = pd.read_csv(\"provided_data/datasets/PCT1_updt.csv\", parse_dates= [\"timestamp\"])\n",
    "dataset_1.rename(columns={\"timestamp\":\"date_rec\"}, inplace=True)\n",
    "\n",
    "sensor_df = dataset_1.set_index(\"date_rec\")\n",
    "# Drop NAs only if for a row there are only NANs\n",
    "sensor_df = sensor_df.dropna(how=\"all\")\n",
    "sensor_df = sensor_df[~sensor_df.index.duplicated(keep='first')]\n",
    "\n",
    "sensor_df.to_csv(\"PCT1_for_sanity_checking_1.csv\")\n",
    "\n",
    "save_test_system_dataset(\n",
    "    dataset=sensor_df,\n",
    "    dataset_tag=config.PCT1_DATA_TAG,\n",
    "    output_dataset_name=\"pct1\")\n",
    "\n",
    "convert_and_save_dataset(\n",
    "    dataset=sensor_df,\n",
    "    dataset_tag=config.PCT1_DATA_TAG,\n",
    "    output_dataset_name=config.PCT1_DATA_TAG,\n",
    "    output_dataset_name_suffix=\"2\",\n",
    "    act_on_na=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCT2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_4 = pd.read_csv(\"provided_data/datasets/PCT2.csv\", parse_dates= [\"timestamp\"])\n",
    "dataset_4.rename(columns={\"timestamp\":\"date_rec\"}, inplace=True)\n",
    "\n",
    "sensor_df = dataset_4.set_index(\"date_rec\")\n",
    "# Drop NAs only if for a row there are only NANs\n",
    "sensor_df = sensor_df.dropna(how=\"all\")\n",
    "sensor_df = sensor_df[~sensor_df.index.duplicated(keep='first')]\n",
    "\n",
    "sensor_df.to_csv(\"PCT2_for_sanity_checking_1.csv\")\n",
    "\n",
    "save_test_system_dataset(\n",
    "    dataset=sensor_df,\n",
    "    dataset_tag=config.PCT2_DATA_TAG,\n",
    "    output_dataset_name=\"pct2\")\n",
    "\n",
    "convert_and_save_dataset(\n",
    "    dataset=sensor_df,\n",
    "    dataset_tag=config.PCT2_DATA_TAG,\n",
    "    output_dataset_name=config.PCT2_DATA_TAG,\n",
    "    output_dataset_name_suffix=\"1\",\n",
    "    act_on_na=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_4 = pd.read_csv(\"provided_data/datasets/PCT2_updt.csv\", parse_dates= [\"timestamp\"])\n",
    "dataset_4.rename(columns={\"timestamp\":\"date_rec\"}, inplace=True)\n",
    "\n",
    "sensor_df = dataset_4.set_index(\"date_rec\")\n",
    "# Drop NAs only if for a row there are only NANs\n",
    "sensor_df = sensor_df.dropna(how=\"all\")\n",
    "sensor_df = sensor_df[~sensor_df.index.duplicated(keep='first')]\n",
    "\n",
    "sensor_df.to_csv(\"PCT2_for_sanity_checking_1.csv\")\n",
    "\n",
    "save_test_system_dataset(\n",
    "    dataset=sensor_df,\n",
    "    dataset_tag=config.PCT2_DATA_TAG,\n",
    "    output_dataset_name=\"pct2\")\n",
    "\n",
    "convert_and_save_dataset(\n",
    "    dataset=sensor_df,\n",
    "    dataset_tag=config.PCT2_DATA_TAG,\n",
    "    output_dataset_name=config.PCT2_DATA_TAG,\n",
    "    output_dataset_name_suffix=\"2\",\n",
    "    act_on_na=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBG 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('original_format/efficiency_modelling/RBG1/rbg_1_efficiency_model.pickle', 'rb') as handle:\n",
    "#    rbg_1_efficiency_calculator = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2 = pd.read_csv(\"provided_data/datasets/RBG1/UJRBG2 2022-06-20.csv\", parse_dates= [\"timestamp\"])\n",
    "dataset_2.rename(columns={\"timestamp\":\"date_rec\"}, inplace=True)\n",
    "\n",
    "sensor_df = dataset_2.set_index(\"date_rec\")\n",
    "# Drop NAs only if for a row there are only NANs\n",
    "sensor_df = sensor_df.dropna(how=\"all\")\n",
    "sensor_df = sensor_df[~sensor_df.index.duplicated(keep='first')]\n",
    "\n",
    "sensor_df.to_csv(\"RBG1_for_sanity_checking.csv\")\n",
    "\n",
    "save_test_system_dataset(\n",
    "    dataset=sensor_df,\n",
    "    dataset_tag=config.RBG1_DATA_TAG,\n",
    "    output_dataset_name=\"rbg1\")\n",
    "\n",
    "convert_and_save_dataset(\n",
    "    dataset=sensor_df,\n",
    "    dataset_tag=config.RBG1_DATA_TAG,\n",
    "    output_dataset_name=config.RBG1_DATA_TAG,\n",
    "    output_dataset_name_suffix=\"2\",\n",
    "    act_on_na=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBG 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('original_format/efficiency_modelling/RBG2/rbg_1_efficiency_model.pickle', 'rb') as handle:\n",
    "#    rbg_3_efficiency_calculator = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_3 = pd.read_csv(\"provided_data/datasets/RBG2.csv\", parse_dates= [\"timestamp\"])\n",
    "dataset_3.rename(columns={\"timestamp\":\"date_rec\"}, inplace=True)\n",
    "\n",
    "sensor_df = dataset_3.set_index(\"date_rec\")\n",
    "# Drop NAs only if for a row there are only NANs\n",
    "sensor_df = sensor_df.dropna(how=\"all\")\n",
    "sensor_df = sensor_df[~sensor_df.index.duplicated(keep='first')]\n",
    "\n",
    "sensor_df.to_csv(\"RBG2_for_sanity_checking.csv\")\n",
    "\n",
    "save_test_system_dataset(\n",
    "    dataset=sensor_df,\n",
    "    dataset_tag=config.RBG2_DATA_TAG,\n",
    "    output_dataset_name=\"rbg2\")\n",
    "\n",
    "convert_and_save_dataset(\n",
    "    dataset=sensor_df,\n",
    "    dataset_tag=config.RBG2_DATA_TAG,\n",
    "    output_dataset_name=config.RBG2_DATA_TAG,\n",
    "    output_dataset_name_suffix=\"2\",\n",
    "    act_on_na=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KTT 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KTT 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19663/240175838.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0msensor_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msensor_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0msensor_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'first'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0msensor_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"KTT1_for_sanity_checking.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m save_test_system_dataset(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3385\u001b[0m         )\n\u001b[1;32m   3386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3387\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3388\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3389\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1081\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m         )\n\u001b[0;32m-> 1083\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    246\u001b[0m             )\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_need_to_save_header\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_body\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstart_i\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0mix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslicer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_native_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_number_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0mlibwriters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_csv_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mpandas/_libs/writers.pyx\u001b[0m in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset_5 = concatenate_daily(\"provided_data/datasets/KTT1/UJKTT_1/model_data\")\n",
    "dataset_5[\"f_timestamp\"] = pd.to_datetime(dataset_5[\"f_timestamp\"]) \n",
    "dataset_5 = dataset_5.sort_values(by=\"f_timestamp\")\n",
    "dataset_5.rename(columns={\"f_timestamp\":\"date_rec\"}, inplace=True)\n",
    "\n",
    "sensor_df = dataset_5.set_index(\"date_rec\")\n",
    "# Drop NAs only if for a row there are only NANs\n",
    "sensor_df = sensor_df.dropna(how=\"all\")\n",
    "sensor_df = sensor_df[~sensor_df.index.duplicated(keep='first')]\n",
    "\n",
    "sensor_df.to_csv(\"KTT1_for_sanity_checking.csv\")\n",
    "\n",
    "save_test_system_dataset(\n",
    "    dataset=sensor_df,\n",
    "    dataset_tag=config.KTT1_DATA_TAG,\n",
    "    output_dataset_name=\"ktt1\")\n",
    "\n",
    "convert_and_save_dataset(\n",
    "    dataset=sensor_df,\n",
    "    dataset_tag=config.KTT1_DATA_TAG,\n",
    "    output_dataset_name=config.KTT1_DATA_TAG,\n",
    "    output_dataset_name_suffix=\"1\",\n",
    "    act_on_na=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KTT 1.2 (compensating for KALTIM1.SIGNAL.AI.10HBK31CQ101 unavailability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_7 = concatenate_daily(\"provided_data/datasets/KTT1/KTT1_additional_data_1/KTT1_13022022_12042022\")\n",
    "dataset_7[\"f_timestamp\"] = pd.to_datetime(dataset_7[\"f_timestamp\"]) \n",
    "dataset_7 = dataset_7.sort_values(by=\"f_timestamp\")\n",
    "dataset_7.rename(columns={\"f_timestamp\":\"date_rec\"}, inplace=True)\n",
    "sensor_df = dataset_7.set_index(\"date_rec\")\n",
    "\n",
    "# Drop NAs only if for a row there are only NANs\n",
    "sensor_df = sensor_df.dropna(how=\"all\")\n",
    "sensor_df = sensor_df[~sensor_df.index.duplicated(keep='first')]\n",
    "\n",
    "sensor_df.to_csv(\"KTT1_for_sanity_checking_2.csv\")\n",
    "\n",
    "save_test_system_dataset(\n",
    "    dataset=sensor_df,\n",
    "    dataset_tag=config.KTT1_DATA_TAG,\n",
    "    output_dataset_name=\"ktt1\")\n",
    "\n",
    "convert_and_save_dataset(\n",
    "    dataset=sensor_df,\n",
    "    dataset_tag=config.KTT1_DATA_TAG,\n",
    "    output_dataset_name=config.KTT1_DATA_TAG,\n",
    "    output_dataset_name_suffix=\"2\",\n",
    "    act_on_na=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KTT 1.3 (combination of the two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KTT_11 = pd.read_csv(\"prepared_datasets/dataset_KTT1_1.csv\")\n",
    "KTT_12 = pd.read_csv(\"prepared_datasets/dataset_KTT1_2.csv\")\n",
    "dataset_8 = pd.concat([KTT_11, KTT_12], ignore_index=True)\n",
    "dataset_8.to_csv(\"prepared_datasets/dataset_KTT1_3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KTT 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KTT 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_6 = concatenate_daily(\"provided_data/datasets/KTT2/KTT2_additional_data_1/KTT2_13022022_12042022\")\n",
    "dataset_6[\"f_timestamp\"] = pd.to_datetime(dataset_6[\"f_timestamp\"]) \n",
    "dataset_6 = dataset_6.sort_values(by=\"f_timestamp\")\n",
    "dataset_6.rename(columns={\"f_timestamp\":\"date_rec\"}, inplace=True)\n",
    "\n",
    "sensor_df = dataset_6.set_index(\"date_rec\")\n",
    "# Drop NAs only if for a row there are only NANs\n",
    "sensor_df = sensor_df.dropna(how=\"all\")\n",
    "sensor_df = sensor_df[~sensor_df.index.duplicated(keep='first')]\n",
    "\n",
    "sensor_df.to_csv(\"KTT2_for_sanity_checking.csv\")\n",
    "\n",
    "save_test_system_dataset(\n",
    "    dataset=sensor_df,\n",
    "    dataset_tag=config.KTT2_DATA_TAG,\n",
    "    output_dataset_name=\"ktt2\")\n",
    "\n",
    "convert_and_save_dataset(\n",
    "    dataset=sensor_df,\n",
    "    dataset_tag=config.KTT2_DATA_TAG,\n",
    "    output_dataset_name=config.KTT2_DATA_TAG,\n",
    "    output_dataset_name_suffix=\"1\",\n",
    "    act_on_na=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KTT 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_11 = concatenate_daily(\"provided_data/datasets/KTT2/UJKTT_2/model_data\")\n",
    "dataset_11[\"f_timestamp\"] = pd.to_datetime(dataset_11[\"f_timestamp\"]) \n",
    "dataset_11 = dataset_11.sort_values(by=\"f_timestamp\")\n",
    "dataset_11.rename(columns={\"f_timestamp\":\"date_rec\"}, inplace=True)\n",
    "\n",
    "sensor_df = dataset_11.set_index(\"date_rec\")\n",
    "# Drop NAs only if for a row there are only NANs\n",
    "sensor_df = sensor_df.dropna(how=\"all\")\n",
    "sensor_df = sensor_df[~sensor_df.index.duplicated(keep='first')]\n",
    "\n",
    "sensor_df.to_csv(\"KTT2_for_sanity_checking.csv\")\n",
    "\n",
    "save_test_system_dataset(\n",
    "    dataset=sensor_df,\n",
    "    dataset_tag=config.KTT2_DATA_TAG,\n",
    "    output_dataset_name=\"ktt2\")\n",
    "\n",
    "convert_and_save_dataset(\n",
    "    dataset=sensor_df,\n",
    "    dataset_tag=config.KTT2_DATA_TAG,\n",
    "    output_dataset_name=config.KTT2_DATA_TAG,\n",
    "    output_dataset_name_suffix=\"2\",\n",
    "    act_on_na=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KTT 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "KTT_21 = pd.read_csv(\"prepared_datasets/dataset_KTT2_1.csv\")\n",
    "KTT_22 = pd.read_csv(\"prepared_datasets/dataset_KTT2_2.csv\")\n",
    "dataset_12 = pd.concat([KTT_21, KTT_22], ignore_index=True)\n",
    "dataset_12.to_csv(\"prepared_datasets/dataset_KTT2_3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KTT 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_13 = concatenate_daily(\"provided_data/datasets/KTT2/KTT2_20220904_20220923\")\n",
    "dataset_13[\"f_timestamp\"] = pd.to_datetime(dataset_13[\"f_timestamp\"]) \n",
    "dataset_13 = dataset_13.sort_values(by=\"f_timestamp\")\n",
    "dataset_13.rename(columns={\"f_timestamp\":\"date_rec\"}, inplace=True)\n",
    "\n",
    "sensor_df = dataset_13.set_index(\"date_rec\")\n",
    "# Drop NAs only if for a row there are only NANs\n",
    "sensor_df = sensor_df.dropna(how=\"all\")\n",
    "sensor_df = sensor_df[~sensor_df.index.duplicated(keep='first')]\n",
    "\n",
    "sensor_df.to_csv(\"KTT2_for_sanity_checking_4.csv\")\n",
    "\n",
    "save_test_system_dataset(\n",
    "    dataset=sensor_df,\n",
    "    dataset_tag=config.KTT2_DATA_TAG,\n",
    "    output_dataset_name=\"ktt2\")\n",
    "\n",
    "convert_and_save_dataset(\n",
    "    dataset=sensor_df,\n",
    "    dataset_tag=config.KTT2_DATA_TAG,\n",
    "    output_dataset_name=config.KTT2_DATA_TAG,\n",
    "    output_dataset_name_suffix=\"4\",\n",
    "    act_on_na=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KTT 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "KTT_23 = pd.read_csv(\"prepared_datasets/dataset_KTT2_3.csv\")\n",
    "KTT_24 = pd.read_csv(\"prepared_datasets/dataset_KTT2_4.csv\")\n",
    "dataset_14 = pd.concat([KTT_23, KTT_24], ignore_index=True)\n",
    "dataset_14.to_csv(\"prepared_datasets/dataset_KTT2_5.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMG 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_9 = pd.read_csv(\"provided_data/datasets/AMG1/AMG1_new.csv\", parse_dates=[\"timestamp\"])\n",
    "pa_pressure_dataset_9 = pd.read_csv(\"provided_data/datasets/AMG1/ALA30CP101.csv\", parse_dates=[\"timestamp\"])\n",
    "pa_pressure_dataset_9 = pa_pressure_dataset_9.sort_values(by=\"timestamp\")\n",
    "dataset_9 = dataset_9.sort_values(by=\"timestamp\")\n",
    "dataset_9[\"ALA30CP101\"] = pa_pressure_dataset_9[\"ALA30CP101\"]\n",
    "dataset_9.rename(columns={\"timestamp\":\"date_rec\"}, inplace=True)\n",
    "\n",
    "sensor_df = dataset_9.set_index(\"date_rec\")\n",
    "#Drop NAs only if for a row there are only NANs\n",
    "sensor_df = sensor_df.dropna(how=\"all\")\n",
    "sensor_df = sensor_df[~sensor_df.index.duplicated(keep='first')]\n",
    "\n",
    "sensor_df.to_csv(\"AMG1_for_sanity_checking.csv\")\n",
    "\n",
    "save_test_system_dataset(\n",
    "   dataset=sensor_df,\n",
    "   dataset_tag=config.AMG1_DATA_TAG,\n",
    "   output_dataset_name=\"amg1\")\n",
    "\n",
    "convert_and_save_dataset(\n",
    "   dataset=sensor_df,\n",
    "   dataset_tag=config.AMG1_DATA_TAG,\n",
    "   output_dataset_name=config.AMG1_DATA_TAG,\n",
    "   output_dataset_name_suffix=\"2\",\n",
    "   act_on_na=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df[\"ALA84CF1_1\"].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMG 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_10 = pd.read_csv(\"provided_data/datasets/AMG2/AMG2_new.csv\", parse_dates=[\"timestamp\"])\n",
    "pa_pressure_dataset_10 = pd.read_csv(\"provided_data/datasets/AMG2/BLA30CP101.csv\", parse_dates=[\"timestamp\"]) \n",
    "pa_pressure_dataset_10 = pa_pressure_dataset_10.sort_values(by=\"timestamp\")\n",
    "dataset_10 = dataset_10.sort_values(by=\"timestamp\")\n",
    "dataset_10[\"BLA30CP101\"] = pa_pressure_dataset_10[\"BLA30CP101\"]\n",
    "dataset_10.rename(columns={\"timestamp\":\"date_rec\"}, inplace=True)\n",
    "\n",
    "sensor_df = dataset_10.set_index(\"date_rec\")\n",
    "# Drop NAs only if for a row there are only NANs\n",
    "sensor_df = sensor_df.dropna(how=\"all\")\n",
    "sensor_df = sensor_df[~sensor_df.index.duplicated(keep='first')]\n",
    "\n",
    "sensor_df.to_csv(\"AMG2_for_sanity_checking.csv\")\n",
    "\n",
    "save_test_system_dataset(\n",
    "    dataset=sensor_df,\n",
    "    dataset_tag=config.AMG2_DATA_TAG,\n",
    "    output_dataset_name=\"amg2\")\n",
    "\n",
    "convert_and_save_dataset(\n",
    "    dataset=sensor_df,\n",
    "    dataset_tag=config.AMG2_DATA_TAG,\n",
    "    output_dataset_name=config.AMG2_DATA_TAG,\n",
    "    output_dataset_name_suffix=\"2\",\n",
    "    act_on_na=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2c2cacdd71460b678b0adf3b8e6cc46da0cf8f0fb5f5431a3c633094833cf96f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
